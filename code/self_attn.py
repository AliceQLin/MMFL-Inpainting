import torch
import torch.nn.init as init
import torch.nn as nn
import torch.nn.functional as F

def init_conv(conv, glu=True):
    init.xavier_uniform_(conv.weight)
    if conv.bias is not None:
        conv.bias.data.zero_()

class SelfAttention(nn.Module):

    def __init__(self, in_dim, activation=F.relu):
        super(SelfAttention, self).__init__()
        self.chanel_in = in_dim
        self.activation = activation
        
        self.f = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8 , kernel_size=1)
        self.g = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8 , kernel_size=1)
        
        self.gamma = nn.Parameter(torch.zeros(1))

        self.softmax  = nn.Softmax(dim=-1)

        init_conv(self.f)
        init_conv(self.g)
        
    def forward(self, x, t):

        m_batchsize, C, width, height = x.size()
        
        f = self.f(x).view(m_batchsize, -1, width * height)
        g = self.g(x).view(m_batchsize, -1, width * height)
        
        attention = torch.bmm(f.permute(0, 2, 1), g) 
        attention = self.softmax(attention)
         
        out = torch.bmm(attention, t)
        out = self.gamma * out + t
        
        return out
